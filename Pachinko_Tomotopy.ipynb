{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a949196-c94d-452f-b580-e87ae04788f3",
   "metadata": {},
   "source": [
    "### Load Libraries and Tools Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03f7f92-e2c0-4092-b5b6-0b622efd5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomotopy\n",
    "import tomotopy as tp\n",
    "\n",
    "# Toolkit\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "# NLTK\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases,CoherenceModel\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0defbf-73e7-4d00-94d1-2721f292ba04",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cceda34b-cf06-4c68-93a6-33ae5280fad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UPDATED: The Roy has slipped and they can't ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went to renew my drivers license at this loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Much has been written about Olive Garden and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've been coming here since July. I had never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I just recently moved to Scottsdale, AZ from C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body\n",
       "0  UPDATED: The Roy has slipped and they can't ge...\n",
       "1  I went to renew my drivers license at this loc...\n",
       "2  Much has been written about Olive Garden and h...\n",
       "3  I've been coming here since July. I had never ...\n",
       "4  I just recently moved to Scottsdale, AZ from C..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/random_sample_data.csv', sep=',', header=0)\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e8ee7-fb24-43ce-bcd2-382afab0f1c2",
   "metadata": {},
   "source": [
    "### Preprocessing Function Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507b81bd-a8e3-404d-91b0-b6d6e6c8dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocess(df, min_token_len=3, rm_accent=True, bigram_min_cnt=5, bigram_thresh=100,\n",
    "                   extra_stops=['from','subject','re', 'edu','use'],\n",
    "                   postags=['NOUN','VERB','ADV','ADJ']):\n",
    "\n",
    "    '''Function wrapper to preprocess the dataset and generate ready to model results\n",
    "    \n",
    "    *** Inputs**\n",
    "    df: dataframe with \"review_body\" column containing text inputs\n",
    "    min_token_len: int -> tokens less than this number are excluded during tokenization\n",
    "    rm_accent : bool -> flag whether to remove deaccents\n",
    "    bigram_min_cnt: int -> ignore all words and bigrams with total collected count lower than this value\n",
    "    bigram_thresh: int -> threshold for building phrases, higher means fewer phrases\n",
    "    extra_stops: list -> extra stopwords to ignore asidr from NLTK default\n",
    "    postags:list -> words/bigrams to include based on POS (part-of-speech)\n",
    "    \n",
    "    ** Returns**\n",
    "    df: Master df with data and labels\n",
    "    word_list_lemmatized: list -> list of lists w/ lemmatized bigrams \n",
    "    '''\n",
    "    \n",
    "    ### Setting up stopwords and Spacy\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    st_words = stopwords.words('english')\n",
    "    st_words.extend(extra_stops)\n",
    "\n",
    "    # Convert values to list\n",
    "    doc_list = df.review_body.values.tolist()\n",
    "\n",
    "    # Remove email signs, newlines, single quotes\n",
    "    doc_list = [re.sub(r'\\S*@\\S*\\s?', '', txt) for txt in doc_list]\n",
    "    doc_list = [re.sub(r'\\s+', ' ', txt) for txt in doc_list]\n",
    "    doc_list = [re.sub(r\"\\'\", \"\", txt) for txt in doc_list]\n",
    "\n",
    "    # Tokenize based on min_token_len and deaccent flags\n",
    "    print(\"Tokenizing...\\n\")\n",
    "    word_list = [simple_preprocess(txt, deacc=rm_accent, min_len=min_token_len) for txt in doc_list]\n",
    "     \n",
    "    # Create bigram models\n",
    "    bigram = Phrases(word_list, min_count=bigram_min_cnt, threshold=bigram_thresh) # use original wordlist to build model\n",
    "    bigram_model = Phraser(bigram)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    print(\"Removing Stopwords...\\n\")\n",
    "    word_list_nostops = [[word for word in txt if word not in st_words] for txt in word_list]\n",
    "    \n",
    "    # Implement bigram models\n",
    "    print(\"Create bigrams...\\n\")\n",
    "    word_bigrams = [bigram_model[w_vec] for w_vec in word_list_nostops] # implement it in the list w/ no stopwords\n",
    "    \n",
    "    # Lemmatize POS-tags to keep\n",
    "    print(\"Lemmatizing, keeping \" + \",\".join(postags)+ \" POS tags...\\n\")\n",
    "    word_list_lemmatized = lemmatize(word_bigrams, ptags=postags)\n",
    "\n",
    "    print(\"Done preprocessing \" + str(df.shape[0]) + \" documents\")\n",
    "    return df, word_list_lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee39abf-4455-4b65-8071-f0abacb73d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function    \n",
    "def lemmatize(word_list, ptags):\n",
    "    '''Lemmatizes words based on allowed postags, input format is list of sublists \n",
    "       with strings'''\n",
    "    spC = spacy.load('en_core_web_sm')\n",
    "    lem_lists =[]\n",
    "    for vec in word_list:\n",
    "        sentence = spC(\" \".join(vec))\n",
    "        lem_lists.append([token.lemma_ for token in sentence if token.pos_ in ptags])\n",
    "    # Remove Empty Lists after Lemmatizing\n",
    "    lem_lists = list(filter(None, lem_lists))\n",
    "    return lem_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c2c25-7739-4f8b-ac09-a070e8c5b4d7",
   "metadata": {},
   "source": [
    "### Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cebe176-972d-49cd-b798-dcd67b21c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n",
      "Removing Stopwords...\n",
      "\n",
      "Create bigrams...\n",
      "\n",
      "Lemmatizing, keeping NOUN,VERB,ADV,ADJ POS tags...\n",
      "\n",
      "Done preprocessing 670866 documents\n"
     ]
    }
   ],
   "source": [
    "df, word_list_lemmatized = run_preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267f1956-7819-4687-a5c0-c2ae7ab04eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['update', 'slip', 'can', 'pop', 'recently', 'post', 'baby', 'stroller', 'quick', 'drink', 'take', 'bartender', 'minute', 'take', 'order', 'find', 'odd', 'considering', 'may', 'people', 'place', 'drink', 'selection', 'food', 'selection', 'bore', 'baby', 'stroller', 'signage', 'could', 'do', 'totally', 'different', 'way', 'due', 'limited', 'space', 'ask', 'people', 'leave', 'baby', 'stroller', 'least', 'give', 'people', 'option', 'piss', 'sign', 'would', 'thanking', 'patron', 'come', 'spend', 'money', 'establishment', 'would', 'want', 'offend', 'people', 'resident', 'area', 'need', 'new', 'serve', 'traditional', 'irish', 'fare', 'reasonable', 'price', 'open', 'great', 'job', 'make', 'former', 'digsof', 'soon', 'walk', 'feel', 'home', 'welcome', 'many', 'place', 'day', 'owner', 'actually', 'greet', 'soon', 'walk', 'issue', 'would', 'comment', 'lack', 'space', 'front', 'bar', 'could', 'configured', 'different', 'way', 'would', 'allowed', 'room', 'stool', 'bar', 'lead', 'comment', 'authentic', 'standing', 'room', 'many', 'guinnesss', 'always', 'nice', 'stool', 'see', 'cheer']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UPDATED: The Roy has slipped and they can't ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went to renew my drivers license at this loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Much has been written about Olive Garden and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've been coming here since July. I had never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I just recently moved to Scottsdale, AZ from C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body\n",
       "0  UPDATED: The Roy has slipped and they can't ge...\n",
       "1  I went to renew my drivers license at this loc...\n",
       "2  Much has been written about Olive Garden and h...\n",
       "3  I've been coming here since July. I had never ...\n",
       "4  I just recently moved to Scottsdale, AZ from C..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview Processed Data\n",
    "print(word_list_lemmatized[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aeb2ed-cad5-4da3-a263-bced94d5ec4c",
   "metadata": {},
   "source": [
    "### Build PAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f104c6-0e1f-4733-9859-c99544141ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Train the HDP Model\n",
    "def train_Pachinko_model(k1, k2, rm_top, min_cf, word_list, iterations, top_n=10):\n",
    "    '''Wrapper function to train tomotopy HDP Model object\n",
    "    \n",
    "    *** Inputs**\n",
    "    k1: int -> number of super-topics(document-topic frequency)\n",
    "    k2: int -> number of sub topics (word-topic frequency)\n",
    "    rm_top: int -> number of top words to be removed. default is 0\n",
    "    min_cf: int -> minimum frequency of words. Those less than min_cf are excluded\n",
    "    word_list: list -> lemmatized word list of lists\n",
    "    iterations : int -> number of iterations, in increments of 10, to train the model\n",
    "    top_n: int -> number of keywords to generate per sub-topic\n",
    "    \n",
    "    ** Returns**\n",
    "    PAM: trained PAM Model \n",
    "    '''\n",
    "    PAM = tp.PAModel(k1=k1,\n",
    "                     k2=k2,\n",
    "                     rm_top=rm_top,\n",
    "                     min_cf=min_cf\n",
    "                     )\n",
    "    # Add docs to train\n",
    "    for vec in word_list:\n",
    "        PAM.add_doc(vec)\n",
    "\n",
    "    print('Starting training model')\n",
    "    for i in range(0, 1000, iterations):\n",
    "        PAM.train(iterations)\n",
    "        print('Iteration: {}\\tLog-likelihood: {}'.format(i, PAM.ll_per_word))\n",
    "    for k in range(PAM.k1):\n",
    "        subtopics = PAM.get_sub_topics(k)\n",
    "        print('\\n\\nSubtopics of topic #%s' % k)\n",
    "        for subtopic, probability in subtopics:\n",
    "            print('Top 10 words of subtopic topic #%s: probability in supertopic #%s: %r' % (subtopic, k, probability))\n",
    "            print('%r' % PAM.get_topic_words(subtopic, top_n=top_n))\n",
    "        \n",
    "    print(\"Done\\n\")  \n",
    "    return PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30505b9c-91d8-4cc5-a573-eb53cb82f412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training model\n",
      "Iteration: 0\tLog-likelihood: -10.139930041875736\n",
      "Iteration: 10\tLog-likelihood: -9.487160654009745\n",
      "Iteration: 20\tLog-likelihood: -9.366111707817879\n",
      "Iteration: 30\tLog-likelihood: -9.349898029101945\n",
      "Iteration: 40\tLog-likelihood: -9.34554649224123\n",
      "Iteration: 50\tLog-likelihood: -9.344710931362744\n",
      "Iteration: 60\tLog-likelihood: -9.344964528040455\n",
      "Iteration: 70\tLog-likelihood: -9.344842011395947\n",
      "Iteration: 80\tLog-likelihood: -9.34437556670947\n",
      "Iteration: 90\tLog-likelihood: -9.34419823489943\n",
      "Iteration: 100\tLog-likelihood: -9.343431213578812\n",
      "Iteration: 110\tLog-likelihood: -9.342986520735392\n",
      "Iteration: 120\tLog-likelihood: -9.34249805384473\n",
      "Iteration: 130\tLog-likelihood: -9.34117902017156\n",
      "Iteration: 140\tLog-likelihood: -9.34108757908582\n",
      "Iteration: 150\tLog-likelihood: -9.3396596257298\n",
      "Iteration: 160\tLog-likelihood: -9.339401235201548\n",
      "Iteration: 170\tLog-likelihood: -9.338533200431389\n",
      "Iteration: 180\tLog-likelihood: -9.337970612794223\n",
      "Iteration: 190\tLog-likelihood: -9.337377542467673\n",
      "Iteration: 200\tLog-likelihood: -9.337018632603137\n",
      "Iteration: 210\tLog-likelihood: -9.335162322885315\n",
      "Iteration: 220\tLog-likelihood: -9.335034751005555\n",
      "Iteration: 230\tLog-likelihood: -9.333934353161652\n",
      "Iteration: 240\tLog-likelihood: -9.333504009537593\n",
      "Iteration: 250\tLog-likelihood: -9.332468311030823\n",
      "Iteration: 260\tLog-likelihood: -9.331697657707977\n",
      "Iteration: 270\tLog-likelihood: -9.331136275461485\n",
      "Iteration: 280\tLog-likelihood: -9.33111920434584\n",
      "Iteration: 290\tLog-likelihood: -9.330457053166096\n",
      "Iteration: 300\tLog-likelihood: -9.328825966562183\n",
      "Iteration: 310\tLog-likelihood: -9.32714274620889\n",
      "Iteration: 320\tLog-likelihood: -9.325389989497697\n",
      "Iteration: 330\tLog-likelihood: -9.324096507209413\n",
      "Iteration: 340\tLog-likelihood: -9.32330617033863\n",
      "Iteration: 350\tLog-likelihood: -9.322214434825618\n",
      "Iteration: 360\tLog-likelihood: -9.320363252999822\n",
      "Iteration: 370\tLog-likelihood: -9.318561736965448\n",
      "Iteration: 380\tLog-likelihood: -9.315758326934617\n",
      "Iteration: 390\tLog-likelihood: -9.315254499411258\n",
      "Iteration: 400\tLog-likelihood: -9.314388454179069\n",
      "Iteration: 410\tLog-likelihood: -9.313374921766073\n",
      "Iteration: 420\tLog-likelihood: -9.312064121227019\n",
      "Iteration: 430\tLog-likelihood: -9.309986227958973\n",
      "Iteration: 440\tLog-likelihood: -9.309756442903549\n",
      "Iteration: 450\tLog-likelihood: -9.309089608460726\n",
      "Iteration: 460\tLog-likelihood: -9.307423644638133\n",
      "Iteration: 470\tLog-likelihood: -9.305210952784426\n",
      "Iteration: 480\tLog-likelihood: -9.305524061186272\n",
      "Iteration: 490\tLog-likelihood: -9.30518788952556\n",
      "Iteration: 500\tLog-likelihood: -9.304780166562317\n",
      "Iteration: 510\tLog-likelihood: -9.30299679301963\n",
      "Iteration: 520\tLog-likelihood: -9.301909545282898\n",
      "Iteration: 530\tLog-likelihood: -9.301774154888307\n",
      "Iteration: 540\tLog-likelihood: -9.300910469979991\n",
      "Iteration: 550\tLog-likelihood: -9.301152842908527\n",
      "Iteration: 560\tLog-likelihood: -9.301742866836344\n",
      "Iteration: 570\tLog-likelihood: -9.300105947822388\n",
      "Iteration: 580\tLog-likelihood: -9.300157285093077\n",
      "Iteration: 590\tLog-likelihood: -9.2997174739154\n",
      "Iteration: 600\tLog-likelihood: -9.29940521876807\n",
      "Iteration: 610\tLog-likelihood: -9.298097566326604\n",
      "Iteration: 620\tLog-likelihood: -9.297404638106828\n",
      "Iteration: 630\tLog-likelihood: -9.298369066587574\n",
      "Iteration: 640\tLog-likelihood: -9.298165177772866\n",
      "Iteration: 650\tLog-likelihood: -9.297588256377885\n",
      "Iteration: 660\tLog-likelihood: -9.29776752856426\n",
      "Iteration: 670\tLog-likelihood: -9.296902615852005\n",
      "Iteration: 680\tLog-likelihood: -9.297190212087786\n",
      "Iteration: 690\tLog-likelihood: -9.29738510638654\n",
      "Iteration: 700\tLog-likelihood: -9.29765711387968\n",
      "Iteration: 710\tLog-likelihood: -9.297023628415907\n",
      "Iteration: 720\tLog-likelihood: -9.297162399199086\n",
      "Iteration: 730\tLog-likelihood: -9.296340908962566\n",
      "Iteration: 740\tLog-likelihood: -9.296515000411976\n",
      "Iteration: 750\tLog-likelihood: -9.296491937420456\n",
      "Iteration: 760\tLog-likelihood: -9.295423201048525\n",
      "Iteration: 770\tLog-likelihood: -9.295020065772402\n",
      "Iteration: 780\tLog-likelihood: -9.295277623638182\n",
      "Iteration: 790\tLog-likelihood: -9.295806083194543\n",
      "Iteration: 800\tLog-likelihood: -9.297085521960662\n",
      "Iteration: 810\tLog-likelihood: -9.29542423518669\n",
      "Iteration: 820\tLog-likelihood: -9.295184661980562\n",
      "Iteration: 830\tLog-likelihood: -9.295327078605512\n",
      "Iteration: 840\tLog-likelihood: -9.294621219908029\n",
      "Iteration: 850\tLog-likelihood: -9.294526476504268\n",
      "Iteration: 860\tLog-likelihood: -9.29384961325575\n",
      "Iteration: 870\tLog-likelihood: -9.294169227221131\n",
      "Iteration: 880\tLog-likelihood: -9.294344880612721\n",
      "Iteration: 890\tLog-likelihood: -9.293047322489906\n",
      "Iteration: 900\tLog-likelihood: -9.292220833661775\n",
      "Iteration: 910\tLog-likelihood: -9.293934070711233\n",
      "Iteration: 920\tLog-likelihood: -9.292683195456313\n",
      "Iteration: 930\tLog-likelihood: -9.292764003842645\n",
      "Iteration: 940\tLog-likelihood: -9.292955337949895\n",
      "Iteration: 950\tLog-likelihood: -9.292913052944847\n",
      "Iteration: 960\tLog-likelihood: -9.293176121309484\n",
      "Iteration: 970\tLog-likelihood: -9.293219428164846\n",
      "Iteration: 980\tLog-likelihood: -9.293797578348375\n",
      "Iteration: 990\tLog-likelihood: -9.293563640262674\n",
      "\n",
      "\n",
      "Subtopics of topic #0\n",
      "Top 10 words of subtopic topic #4: probability in supertopic #0: 0.29774343967437744\n",
      "[('server', 0.007985309697687626), ('meal', 0.007753193378448486), ('dinner', 0.007198446895927191), ('salad', 0.006353068631142378), ('serve', 0.006064557936042547), ('rice', 0.005225189961493015), ('lunch', 0.005174629855901003), ('meat', 0.005039920564740896), ('portion', 0.004617408383637667), ('plate', 0.004525304306298494)]\n",
      "Top 10 words of subtopic topic #0: probability in supertopic #0: 0.2657780647277832\n",
      "[('car', 0.009082858450710773), ('care', 0.0058250329457223415), ('do', 0.005801891442388296), ('help', 0.005711236968636513), ('job', 0.005666292272508144), ('manager', 0.00539853610098362), ('thank', 0.004992503207176924), ('company', 0.00497567281126976), ('guy', 0.004796850029379129), ('hair', 0.004788052290678024)]\n",
      "Top 10 words of subtopic topic #1: probability in supertopic #0: 0.20755097270011902\n",
      "[('hotel', 0.007915666326880455), ('stay', 0.007871336303651333), ('show', 0.006334336474537849), ('store', 0.0058915442787110806), ('fun', 0.004742237739264965), ('play', 0.004234579391777515), ('music', 0.00422906968742609), ('old', 0.003973362036049366), ('kid', 0.0039525749161839485), ('parking', 0.003924274351447821)]\n",
      "Top 10 words of subtopic topic #2: probability in supertopic #0: 0.18132272362709045\n",
      "[('pizza', 0.015741825103759766), ('coffee', 0.010375240817666054), ('cheese', 0.00828607752919197), ('breakfast', 0.008261464536190033), ('sandwich', 0.007934670895338058), ('burger', 0.007767122704535723), ('flavor', 0.005895623937249184), ('delicious', 0.005863597150892019), ('sweet', 0.005408399738371372), ('hot', 0.004952016286551952)]\n",
      "Top 10 words of subtopic topic #3: probability in supertopic #0: 0.04760481417179108\n",
      "[('model', 0.012007505632936954), ('use', 0.011595701798796654), ('set', 0.010258891619741917), ('function', 0.0076463171280920506), ('problem', 0.006808224134147167), ('result', 0.006692339666187763), ('show', 0.006673715077340603), ('learn', 0.006624050438404083), ('method', 0.005763194058090448), ('number', 0.005413471255451441)]\n",
      "\n",
      "\n",
      "Subtopics of topic #1\n",
      "Top 10 words of subtopic topic #4: probability in supertopic #1: 0.29079118371009827\n",
      "[('server', 0.007985309697687626), ('meal', 0.007753193378448486), ('dinner', 0.007198446895927191), ('salad', 0.006353068631142378), ('serve', 0.006064557936042547), ('rice', 0.005225189961493015), ('lunch', 0.005174629855901003), ('meat', 0.005039920564740896), ('portion', 0.004617408383637667), ('plate', 0.004525304306298494)]\n",
      "Top 10 words of subtopic topic #0: probability in supertopic #1: 0.2766657769680023\n",
      "[('car', 0.009082858450710773), ('care', 0.0058250329457223415), ('do', 0.005801891442388296), ('help', 0.005711236968636513), ('job', 0.005666292272508144), ('manager', 0.00539853610098362), ('thank', 0.004992503207176924), ('company', 0.00497567281126976), ('guy', 0.004796850029379129), ('hair', 0.004788052290678024)]\n",
      "Top 10 words of subtopic topic #1: probability in supertopic #1: 0.20773783326148987\n",
      "[('hotel', 0.007915666326880455), ('stay', 0.007871336303651333), ('show', 0.006334336474537849), ('store', 0.0058915442787110806), ('fun', 0.004742237739264965), ('play', 0.004234579391777515), ('music', 0.00422906968742609), ('old', 0.003973362036049366), ('kid', 0.0039525749161839485), ('parking', 0.003924274351447821)]\n",
      "Top 10 words of subtopic topic #2: probability in supertopic #1: 0.16986994445323944\n",
      "[('pizza', 0.015741825103759766), ('coffee', 0.010375240817666054), ('cheese', 0.00828607752919197), ('breakfast', 0.008261464536190033), ('sandwich', 0.007934670895338058), ('burger', 0.007767122704535723), ('flavor', 0.005895623937249184), ('delicious', 0.005863597150892019), ('sweet', 0.005408399738371372), ('hot', 0.004952016286551952)]\n",
      "Top 10 words of subtopic topic #3: probability in supertopic #1: 0.05493529886007309\n",
      "[('model', 0.012007505632936954), ('use', 0.011595701798796654), ('set', 0.010258891619741917), ('function', 0.0076463171280920506), ('problem', 0.006808224134147167), ('result', 0.006692339666187763), ('show', 0.006673715077340603), ('learn', 0.006624050438404083), ('method', 0.005763194058090448), ('number', 0.005413471255451441)]\n",
      "\n",
      "\n",
      "Subtopics of topic #2\n",
      "Top 10 words of subtopic topic #4: probability in supertopic #2: 0.28913888335227966\n",
      "[('server', 0.007985309697687626), ('meal', 0.007753193378448486), ('dinner', 0.007198446895927191), ('salad', 0.006353068631142378), ('serve', 0.006064557936042547), ('rice', 0.005225189961493015), ('lunch', 0.005174629855901003), ('meat', 0.005039920564740896), ('portion', 0.004617408383637667), ('plate', 0.004525304306298494)]\n",
      "Top 10 words of subtopic topic #0: probability in supertopic #2: 0.26931679248809814\n",
      "[('car', 0.009082858450710773), ('care', 0.0058250329457223415), ('do', 0.005801891442388296), ('help', 0.005711236968636513), ('job', 0.005666292272508144), ('manager', 0.00539853610098362), ('thank', 0.004992503207176924), ('company', 0.00497567281126976), ('guy', 0.004796850029379129), ('hair', 0.004788052290678024)]\n",
      "Top 10 words of subtopic topic #1: probability in supertopic #2: 0.2121240347623825\n",
      "[('hotel', 0.007915666326880455), ('stay', 0.007871336303651333), ('show', 0.006334336474537849), ('store', 0.0058915442787110806), ('fun', 0.004742237739264965), ('play', 0.004234579391777515), ('music', 0.00422906968742609), ('old', 0.003973362036049366), ('kid', 0.0039525749161839485), ('parking', 0.003924274351447821)]\n",
      "Top 10 words of subtopic topic #2: probability in supertopic #2: 0.18135657906532288\n",
      "[('pizza', 0.015741825103759766), ('coffee', 0.010375240817666054), ('cheese', 0.00828607752919197), ('breakfast', 0.008261464536190033), ('sandwich', 0.007934670895338058), ('burger', 0.007767122704535723), ('flavor', 0.005895623937249184), ('delicious', 0.005863597150892019), ('sweet', 0.005408399738371372), ('hot', 0.004952016286551952)]\n",
      "Top 10 words of subtopic topic #3: probability in supertopic #2: 0.04806375876069069\n",
      "[('model', 0.012007505632936954), ('use', 0.011595701798796654), ('set', 0.010258891619741917), ('function', 0.0076463171280920506), ('problem', 0.006808224134147167), ('result', 0.006692339666187763), ('show', 0.006673715077340603), ('learn', 0.006624050438404083), ('method', 0.005763194058090448), ('number', 0.005413471255451441)]\n",
      "\n",
      "\n",
      "Subtopics of topic #3\n",
      "Top 10 words of subtopic topic #4: probability in supertopic #3: 0.2998352348804474\n",
      "[('server', 0.007985309697687626), ('meal', 0.007753193378448486), ('dinner', 0.007198446895927191), ('salad', 0.006353068631142378), ('serve', 0.006064557936042547), ('rice', 0.005225189961493015), ('lunch', 0.005174629855901003), ('meat', 0.005039920564740896), ('portion', 0.004617408383637667), ('plate', 0.004525304306298494)]\n",
      "Top 10 words of subtopic topic #0: probability in supertopic #3: 0.27654069662094116\n",
      "[('car', 0.009082858450710773), ('care', 0.0058250329457223415), ('do', 0.005801891442388296), ('help', 0.005711236968636513), ('job', 0.005666292272508144), ('manager', 0.00539853610098362), ('thank', 0.004992503207176924), ('company', 0.00497567281126976), ('guy', 0.004796850029379129), ('hair', 0.004788052290678024)]\n",
      "Top 10 words of subtopic topic #1: probability in supertopic #3: 0.20369547605514526\n",
      "[('hotel', 0.007915666326880455), ('stay', 0.007871336303651333), ('show', 0.006334336474537849), ('store', 0.0058915442787110806), ('fun', 0.004742237739264965), ('play', 0.004234579391777515), ('music', 0.00422906968742609), ('old', 0.003973362036049366), ('kid', 0.0039525749161839485), ('parking', 0.003924274351447821)]\n",
      "Top 10 words of subtopic topic #2: probability in supertopic #3: 0.1694308966398239\n",
      "[('pizza', 0.015741825103759766), ('coffee', 0.010375240817666054), ('cheese', 0.00828607752919197), ('breakfast', 0.008261464536190033), ('sandwich', 0.007934670895338058), ('burger', 0.007767122704535723), ('flavor', 0.005895623937249184), ('delicious', 0.005863597150892019), ('sweet', 0.005408399738371372), ('hot', 0.004952016286551952)]\n",
      "Top 10 words of subtopic topic #3: probability in supertopic #3: 0.05049772560596466\n",
      "[('model', 0.012007505632936954), ('use', 0.011595701798796654), ('set', 0.010258891619741917), ('function', 0.0076463171280920506), ('problem', 0.006808224134147167), ('result', 0.006692339666187763), ('show', 0.006673715077340603), ('learn', 0.006624050438404083), ('method', 0.005763194058090448), ('number', 0.005413471255451441)]\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PAM_mdl = train_Pachinko_model(k1=4, \n",
    "                               k2=5,\n",
    "                               rm_top=100,\n",
    "                               min_cf=50,\n",
    "                               word_list=word_list_lemmatized,\n",
    "                               iterations=10\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ab2a61-1a5e-4e53-b989-4eabec44fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub Topic #0\n",
      "[('car', 0.009082858450710773),\n",
      " ('care', 0.0058250329457223415),\n",
      " ('do', 0.005801891442388296),\n",
      " ('help', 0.005711236968636513),\n",
      " ('job', 0.005666292272508144),\n",
      " ('manager', 0.00539853610098362),\n",
      " ('thank', 0.004992503207176924),\n",
      " ('company', 0.00497567281126976),\n",
      " ('guy', 0.004796850029379129),\n",
      " ('hair', 0.004788052290678024)]\n",
      "\n",
      "\n",
      "Sub Topic #1\n",
      "[('hotel', 0.007915666326880455),\n",
      " ('stay', 0.007871336303651333),\n",
      " ('show', 0.006334336474537849),\n",
      " ('store', 0.0058915442787110806),\n",
      " ('fun', 0.004742237739264965),\n",
      " ('play', 0.004234579391777515),\n",
      " ('music', 0.00422906968742609),\n",
      " ('old', 0.003973362036049366),\n",
      " ('kid', 0.0039525749161839485),\n",
      " ('parking', 0.003924274351447821)]\n",
      "\n",
      "\n",
      "Sub Topic #2\n",
      "[('pizza', 0.015741825103759766),\n",
      " ('coffee', 0.010375240817666054),\n",
      " ('cheese', 0.00828607752919197),\n",
      " ('breakfast', 0.008261464536190033),\n",
      " ('sandwich', 0.007934670895338058),\n",
      " ('burger', 0.007767122704535723),\n",
      " ('flavor', 0.005895623937249184),\n",
      " ('delicious', 0.005863597150892019),\n",
      " ('sweet', 0.005408399738371372),\n",
      " ('hot', 0.004952016286551952)]\n",
      "\n",
      "\n",
      "Sub Topic #3\n",
      "[('model', 0.012007505632936954),\n",
      " ('use', 0.011595701798796654),\n",
      " ('set', 0.010258891619741917),\n",
      " ('function', 0.0076463171280920506),\n",
      " ('problem', 0.006808224134147167),\n",
      " ('result', 0.006692339666187763),\n",
      " ('show', 0.006673715077340603),\n",
      " ('learn', 0.006624050438404083),\n",
      " ('method', 0.005763194058090448),\n",
      " ('number', 0.005413471255451441)]\n",
      "\n",
      "\n",
      "Sub Topic #4\n",
      "[('server', 0.007985309697687626),\n",
      " ('meal', 0.007753193378448486),\n",
      " ('dinner', 0.007198446895927191),\n",
      " ('salad', 0.006353068631142378),\n",
      " ('serve', 0.006064557936042547),\n",
      " ('rice', 0.005225189961493015),\n",
      " ('lunch', 0.005174629855901003),\n",
      " ('meat', 0.005039920564740896),\n",
      " ('portion', 0.004617408383637667),\n",
      " ('plate', 0.004525304306298494)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# View topics per sub-topic - k2 value\n",
    "for i in range(0, PAM_mdl.k2):\n",
    "    subtopic_num = i\n",
    "    print(f\"Sub Topic #{subtopic_num}\")\n",
    "    pprint(PAM_mdl.get_topic_words(subtopic_num))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
