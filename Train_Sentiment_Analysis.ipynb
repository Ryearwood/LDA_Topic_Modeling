{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d83714a8-6735-42bf-8ad7-4d5d65988195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\russe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time\n",
    "import nltk\n",
    "from emoji import demojize\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95dec42-7f69-405b-92f3-1d7b4df8f20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ...\n",
       "2      0  @Kenichan I dived many times for the ball. Man...\n",
       "3      0    my whole body feels itchy and like its on fire \n",
       "4      0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['label', 'tweet'] \n",
    "raw_data = pd.read_csv('data/Datasets/Sentiment140.csv', usecols=columns)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b01508-ede0-4521-8798-1297835aa9a3",
   "metadata": {},
   "source": [
    "### Preprocess the texts:\n",
    "\n",
    "- Lowercase Text Conversion \n",
    "- Special character Removal: links, usernames and transform emojis to text\n",
    "- Repetition removal (e.g. Helloooooo => Hello)\n",
    "- Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572d6eaa-eecf-415b-9ced-7f84045dc23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up: 430.39 sec\n"
     ]
    }
   ],
   "source": [
    "# Time Start\n",
    "start = time()\n",
    "\n",
    "# Raw Text Data\n",
    "texts = raw_data.tweet\n",
    "\n",
    "# Lowercase Text Conversion\n",
    "texts = texts.str.lower()\n",
    "\n",
    "# Special Character Removal\n",
    "texts = texts.str.replace(r\"(http|@)\\S+\", \"\",regex=True)\n",
    "texts = texts.apply(demojize)\n",
    "texts = texts.str.replace(r\"::\", \": :\",regex=True)\n",
    "texts = texts.str.replace(r\"â€™\", \"'\",regex=True)\n",
    "texts = texts.str.replace(r\"[^a-z\\':_]\", \" \",regex=True)\n",
    "\n",
    "# Repetition removal\n",
    "pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "texts = texts.str.replace(pattern, r\"\\1\",regex=True)\n",
    "\n",
    "# Transform short negation form\n",
    "texts = texts.str.replace(r\"(can't|cannot)\", 'can not',regex=True)\n",
    "texts = texts.str.replace(r\"n't\", ' not',regex=True)\n",
    "\n",
    "# Stopword Removal\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "## Keep Negation-Relevant wording\n",
    "stopwords.remove('not')\n",
    "stopwords.remove('nor')\n",
    "stopwords.remove('no')\n",
    "## Apply\n",
    "texts = texts.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "\n",
    "# Time End\n",
    "print(\"Time to clean up: {:.2f} sec\".format(time() - start))\n",
    "\n",
    "# Re-assign cleaned up texts to dataframe column\n",
    "raw_data.tweet = texts\n",
    "clean_data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57572daa-cf94-464a-9a89-edcbede0a540",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "Transform the text corpus to a vector representation\n",
    "\n",
    "- word_count: Number of words to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9673e9-c189-42f4-9abb-0c3420bd66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1205fd7-8421-4554-8623-357061981e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tokenizer with number of words and lowercasing\n",
    "tokenizer = Tokenizer(num_words=word_count, lower=True)\n",
    "# Fit and apply tokenizer to text data\n",
    "tokenizer.fit_on_texts(clean_data.tweet)\n",
    "\n",
    "file_location='data/Pickles/Sentiment140/tokenizer.pkl'\n",
    "with open(file_location,'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a0fc0-f096-4fbb-875d-534adeae317a",
   "metadata": {},
   "source": [
    "### Data Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9a4086-e0a4-4560-b23d-08e38ac0e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframes\n",
    "train = pd.DataFrame(columns=['label', 'tweet'])\n",
    "validation = pd.DataFrame(columns=['label', 'tweet'])\n",
    "\n",
    "# Populate dataframes by unique label values\n",
    "for label in clean_data.label.unique():\n",
    "    label_data = clean_data[clean_data.label == label]\n",
    "    # Equalize distribution by splitting grouped label data\n",
    "    train_data, validation_data = train_test_split(label_data, test_size=0.3)\n",
    "    # Append to respective dataframes\n",
    "    train = pd.concat([train, train_data])\n",
    "    validation = pd.concat([validation, validation_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4857bfb-45ad-44c4-a737-6fe524d4caef",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Model Building - GRU: Bidirectional Layering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7112825b-173a-484a-adb1-2ad30cff7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow-Specific Libraries for Modelling\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU\n",
    "from tensorflow.keras.layers import Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73a6ed9-ba6c-4cec-9d9f-44a35e8baa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Parameters for Use (Fine-Tune as-needed)\n",
    "input_dim = min(tokenizer.num_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 200\n",
    "input_length = 100\n",
    "gru_units = 128\n",
    "gru_dropout = 0.1\n",
    "recurrent_dropout = 0.1\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6a602a-79a4-48e1-ace6-9e818188e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 256)          253440    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,261,697\n",
      "Trainable params: 2,261,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Sequential\n",
    "model = Sequential()\n",
    "# Embedding Layers\n",
    "model.add(Embedding(input_dim=input_dim,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_shape=(input_length,)\n",
    "                    ))\n",
    "# Bidirectional Layer\n",
    "model.add(Bidirectional(GRU(gru_units,\n",
    "                            return_sequences=True,\n",
    "                            dropout=gru_dropout,\n",
    "                            recurrent_dropout=recurrent_dropout)))\n",
    "# Pooling Layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# Dense Layer - Relu Activation\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# Dropout gating\n",
    "model.add(Dropout(dropout))\n",
    "# Dense Layer - Sigmoid Activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf4c9e0-dc65-4e01-938c-c27eb79d6df1",
   "metadata": {},
   "source": [
    "### Prep Input Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024d657e-a7d4-453b-8c9a-3feaf9b1801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ab2515-b43f-431a-b987-ab77d570a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate sentences from prepped data\n",
    "train_sequences = [text.split() for text in train.tweet]\n",
    "validation_sequences = [text.split() for text in validation.tweet]\n",
    "\n",
    "# Tokenize sentences for sequence padding\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_sequences)\n",
    "list_tokenized_validation = tokenizer.texts_to_sequences(validation_sequences)\n",
    "\n",
    "# Pad Sequences\n",
    "x_train = pad_sequences(list_tokenized_train, maxlen=input_length)\n",
    "x_validation = pad_sequences(list_tokenized_validation, maxlen=input_length)\n",
    "\n",
    "# Replace labels\n",
    "y_train = train.label.replace(4, 1)\n",
    "y_validation = validation.label.replace(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c389cad-2d07-43d2-9a4f-06e58bfe5507",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7544c302-bdc4-4cd1-ad08-e6ada3253fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8750/8750 [==============================] - 19186s 2s/step - loss: 0.4357 - accuracy: 0.7976 - val_loss: 0.4147 - val_accuracy: 0.8092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d154c02e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_validation, y_validation)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d27370-2a8d-4469-a3d4-c92ac4a19049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Entire Model including weights and Optimizer settings\n",
    "model_file = 'data/Models/Sentiment_Analysis/gru_model.h5'\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0987463d-ff07-4715-be8d-fe4722218e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 256)          253440    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,261,697\n",
      "Trainable params: 2,261,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "new_model = tf.keras.models.load_model(model_file)\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
